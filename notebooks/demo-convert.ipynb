{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a29764-f39c-431c-8e77-fbc6bfe20f01",
   "metadata": {},
   "source": [
    "# AMOCatlas conversion & compliance checker\n",
    "\n",
    "The purpose of this notebook is to demonstrate the OceanSites format(s) from `AMOCatlas`.\n",
    "\n",
    "The demo is organised to show\n",
    "\n",
    "- Step 1: Loading and plotting a sample dataset\n",
    "\n",
    "- Step 2: Converting one dataset to a standard format\n",
    "\n",
    "Note that when you submit a pull request, you should `clear all outputs` from your python notebook for a cleaner merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1920f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "script_dir = pathlib.Path().parent.absolute()\n",
    "parent_dir = script_dir.parents[0]\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "import importlib\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "from amocatlas import readers, plotters, standardise, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e070d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path for writing datafiles\n",
    "data_path = os.path.join(parent_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414445e",
   "metadata": {},
   "source": [
    "### Load RAPID 26°N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd849c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from data/moc_transports (Quick start)\n",
    "ds_rapid = readers.load_sample_dataset()\n",
    "ds_rapid = standardise.standardise_rapid(ds_rapid, ds_rapid.attrs[\"source_file\"])\n",
    "\n",
    "# Load data from data/moc_transports (Full dataset)\n",
    "datasetsRAPID = readers.load_dataset(\"rapid\", transport_only=True)\n",
    "standardRAPID = [\n",
    "    standardise.standardise_rapid(ds, ds.attrs[\"source_file\"]) for ds in datasetsRAPID\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb527153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RAPID timeseries\n",
    "\n",
    "plotters.plot_amoc_timeseries(\n",
    "    data=[standardRAPID[0]],\n",
    "    varnames=[\"moc_mar_hc10\"],\n",
    "    labels=[\"\"],\n",
    "    resample_monthly=True,\n",
    "    plot_raw=True,\n",
    "    title=\"RAPID 26°N\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fva7rp084v",
   "metadata": {},
   "source": [
    "### Step 2: Convert to AC1 Format\n",
    "\n",
    "The next step is to convert the standardised dataset to AC1 format, which follows OceanSITES conventions.\n",
    "\n",
    "**Note**: This conversion currently fails because the standardise.py step doesn't add proper units to the TIME coordinate. This demonstrates the architectural principle that convert.py validates rather than assigns units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3z0fh11wbpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amocatlas import convert, writers, compliance_checker\n",
    "\n",
    "# Attempt to convert standardised data to AC1 format\n",
    "print(\"🔄 Attempting to convert RAPID data to AC1 format...\")\n",
    "\n",
    "try:\n",
    "    ac1_datasets = convert.to_AC1(standardRAPID[0])\n",
    "    ac1_ds = ac1_datasets[0]\n",
    "    \n",
    "    print(\"✅ Conversion successful!\")\n",
    "    print(f\"  Suggested filename: {ac1_ds.attrs['suggested_filename']}\")\n",
    "    print(f\"  Dimensions: {dict(ac1_ds.dims)}\")\n",
    "    print(f\"  Variables: {list(ac1_ds.data_vars.keys())}\")\n",
    "    \n",
    "    # Save the dataset\n",
    "    output_file = os.path.join(data_path, ac1_ds.attrs['suggested_filename'])\n",
    "    success = writers.save_dataset(ac1_ds, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"💾 Saved AC1 file: {output_file}\")\n",
    "        \n",
    "        # Run compliance check\n",
    "        print(\"\\\\n🔍 Running compliance check...\")\n",
    "        result = compliance_checker.validate_ac1_file(output_file)\n",
    "        \n",
    "        print(f\"Status: {'✅ PASS' if result.passed else '❌ FAIL'}\")\n",
    "        print(f\"Errors: {len(result.errors)}\")\n",
    "        print(f\"Warnings: {len(result.warnings)}\")\n",
    "        \n",
    "        if result.errors:\n",
    "            print(\"\\\\nFirst few errors:\")\n",
    "            for i, error in enumerate(result.errors[:3], 1):\n",
    "                print(f\"  {i}. {error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Conversion failed: {e}\")\n",
    "    print(\"\\\\nThis is expected because standardise.py needs to be updated to provide proper units.\")\n",
    "    print(\"The convert.py module validates that units are present rather than assigning them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotters.show_attributes(ac1_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9kv2x4qlgj",
   "metadata": {},
   "source": [
    "### Demonstration: Working conversion with manual units fix\n",
    "\n",
    "To demonstrate what a successful conversion would look like, let's temporarily fix the TIME units and run the complete workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gx3qn1dhq6s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily fix the TIME units to demonstrate successful conversion\n",
    "# (This would normally be done in standardise.py)\n",
    "demo_ds = standardRAPID[0].copy()\n",
    "demo_ds['TIME'].attrs['units'] = 'seconds since 1970-01-01T00:00:00Z'\n",
    "\n",
    "print(\"🔄 Converting RAPID data to AC1 format (with TIME units fixed)...\")\n",
    "\n",
    "try:\n",
    "    ac1_datasets = convert.to_AC1(demo_ds)\n",
    "    ac1_ds = ac1_datasets[0]\n",
    "    \n",
    "    print(\"✅ Conversion successful!\")\n",
    "    print(f\"  Suggested filename: {ac1_ds.attrs['id']}.nc\")\n",
    "    print(f\"  Dimensions: {dict(ac1_ds.sizes)}\")\n",
    "    print(f\"  Variables: {list(ac1_ds.data_vars.keys())}\")\n",
    "    print(f\"  TIME units: {ac1_ds.TIME.attrs.get('units')}\")\n",
    "    print(f\"  TRANSPORT units: {ac1_ds.TRANSPORT.attrs.get('units')}\")\n",
    "    \n",
    "    # Inspect the structure\n",
    "    print(\"\\\\n📊 Dataset structure:\")\n",
    "    print(f\"  TRANSPORT shape: {ac1_ds.TRANSPORT.shape}\")\n",
    "    print(f\"  Component names: {list(ac1_ds.TRANSPORT_NAME.values)}\")\n",
    "    print(f\"  Global attributes: {len(ac1_ds.attrs)} attributes\")\n",
    "    \n",
    "    # Save the dataset using the writers module\n",
    "    output_file = os.path.join(data_path, ac1_ds.attrs['id'] + \".nc\")\n",
    "    print(f\"\\\\n💾 Saving to: {output_file}\")\n",
    "    success = writers.save_dataset(ac1_ds, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✅ Successfully saved AC1 file!\")\n",
    "        \n",
    "        # File size check\n",
    "        file_size = os.path.getsize(output_file)\n",
    "        print(f\"  File size: {file_size:,} bytes\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Failed to save file\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Conversion failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sjfqfe2hmu9",
   "metadata": {},
   "source": [
    "### Step 3: Compliance Checking\n",
    "\n",
    "Run the AC1 compliance checker to validate the converted file against the specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "od4qqe2kz8i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compliance check on the created file\n",
    "if 'output_file' in locals() and os.path.exists(output_file):\n",
    "    print(\"🔍 Running AC1 compliance check...\")\n",
    "    \n",
    "    result = compliance_checker.validate_ac1_file(output_file)\n",
    "    \n",
    "    print(f\"\\\\n📊 Compliance Results:\")\n",
    "    print(f\"  Status: {'✅ PASS' if result.passed else '❌ FAIL'}\")\n",
    "    print(f\"  File Type: {result.file_type}\")\n",
    "    print(f\"  Errors: {len(result.errors)}\")\n",
    "    print(f\"  Warnings: {len(result.warnings)}\")\n",
    "    \n",
    "    if result.errors:\n",
    "        print(f\"\\\\n❌ Errors ({len(result.errors)} total):\")\n",
    "        for i, error in enumerate(result.errors[:5], 1):\n",
    "            print(f\"  {i}. {error}\")\n",
    "        if len(result.errors) > 5:\n",
    "            print(f\"  ... and {len(result.errors) - 5} more errors\")\n",
    "    \n",
    "    if result.warnings:\n",
    "        print(f\"\\\\n⚠️  Warnings ({len(result.warnings)} total):\")\n",
    "        for i, warning in enumerate(result.warnings[:3], 1):\n",
    "            print(f\"  {i}. {warning}\")\n",
    "        if len(result.warnings) > 3:\n",
    "            print(f\"  ... and {len(result.warnings) - 3} more warnings\")\n",
    "    \n",
    "    # Show validation categories\n",
    "    print(f\"\\\\n🔧 What the compliance checker validates:\")\n",
    "    print(\"  ✓ Filename pattern (OceanSITES conventions)\")\n",
    "    print(\"  ✓ Required dimensions and variables\")\n",
    "    print(\"  ✓ Variable attributes (units, standard_name, vocabulary)\")\n",
    "    print(\"  ✓ Global attributes (conventions, metadata)\")\n",
    "    print(\"  ✓ Data value ranges (coordinates, valid_min/max)\")\n",
    "    print(\"  ✓ CF convention compliance (dimension ordering)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No AC1 file available for compliance checking\")\n",
    "    print(\"Please ensure the conversion step above succeeded first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
